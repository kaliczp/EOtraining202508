* Intro
Mark Russwurm
TU Munich geodesy, nem figyeltem.

Bemutatkozás.

Tensorflow an Pytorch
principals and overview, practice

DeepLearning Architecture

Google Colab Exercise. Model Training.

Focus on the training.

* Sources
Teaching books
Shalev-Shwartsz theory

Bishop&Bishop pdf online!

** Topical
Not consistens chapter authors.

* Intro & Overview
RS measure from distance by sat UAV

Structured multi-band

Slides

Abudant data. S2 13 channels = 4TB/day sentinel.

Block of tensors 4D tensors time-series

** Deep Learning
- image classification: input 3D tensor -> 1D tensor
- segmentation 3D -> output 3D tensor
- time-series -> classes.

In 17 Today only supervised learning but. Train self supervised,
patterns downstream taks

** Timeline
- pre 2012 small labelled dataset RF, boosting, lin reg.
- 2015 onward large labelled supervised DL MLPs, CNNs, RNNs, focus on understanding
- 2020 and later self supervised! using and applying

Currendt.

2012 Feature Lerning linear/logistic regression. Learn
feature. Calculatin error function. ML estimation.
Training dataset

Classic ML input tensor Feature design (NDVI,NDwi, GLCM) how design
this feature, complex classifier. Fexible models.

Design knowledge in the training data

Algorithm to data quality.

How to store geospat data? Raster discretised, NN makes areas of species presence.

Cross-model learning

** What we need?
- Knowledge
  - What model?
  - What pre-trained available?
  - Which code/package to use?
  - How to annotate data and train model?
- Data
  - Access of archives
  - Storege cloud computing
  - focus fast data loading
  - acces with labelling software
- Compute
  - GPUs 10* accel
  - Easy access to servers development and debugging!!! HPC
    Everybody go to the cloud!
Today first three subpoint.

Recap.

* Deep Model Architectures
MLPs, CNNs, Transformes

** Overview
- 1950 MLP liear algebra
- 1990 CNN
- 2015 ResNet
- 2020 Transformes

** Multi-layer perceptrons (MLP)
bio vs. lin algebra

Basicly some imputs multiplied by weights.

input vector -> weights -> output vector
linear transformation

Parallel computation! accelerates! GPU with lots of cores.
Deep Learning = linear transformation

Linear projection. to 2D

Opossite also

** Bio example
neuron voltage quick = activated

Logistic/Sigmoid function model High frequency activated function.0 to 1
hyperbolic tangens tanh -1 to 1

** New layer
Non-linearities. Simple and effective Rectified Linear Unit ReLU
ReLU simpler and faster computation. curved surface, good for classification.

** PyTorch
We have three layers. Model code. with ReLU function. a class example.

Andrej Karpathy demo.

Kornél magyarázat.

Look:
https://cs.stanford.edu/people/karpathy/convnetjs/demo/classify2d.html

* Break

* MLP
* 1950
Vectors and neighbourhoods. Image shape.
H*W

One pixel a time.

Convolution is an another layer. Neighborhoods.
Animation gif. Sharpening. Output raster.

1x1 multilayer perceptor. Pixelwise MLP.

Pooling operation underline feature. Reduce image size. 6*6 -> 3*3

ReLU useful. All the negative numbers.

** 1990 CNN
Increased hidden layers. pulling convulution. Different layers, different
convolution. Every layers compose shapes.

A 48-49-es dián talán hibás a számolás nem OK.

** Batch Normalisation and Skip Connection
Feature space in different points. Linear transformation.

Deeper layer cross dependencies. Normalisation avoids
covariation. Re-centering data.

Add original to output skip connections. If something zeroed it gives the original.
** ResNets 2015
Large number of layers. Complex patterns. Simple only download inilisalisation.
* Classification to segmentation

CNN classification pixel space.

Segmentation separate class.

CNN classification result tree.

Segmentation remove layer averaging. Reduced image we have feature map.
Problem we loose spatial resolution in each step. Coarse image.

Solution: Fully Convolutional Networks high res. feature map.
Pooling removed. Have large receptive field.

Upsampling with CNN decoder
Skip pooling and encoder reduce to smaller, upsample small image.
Problem is that to low res pooling OK. upscaling loose details.

Enc.-dec. model with pooling and upsampling
** U-Net
U-Net idea: Sip Connections 2015
Skip connection add high resolution to unet. Encoder and decoder.
Still bring information patterns. What class what pixel?
For RS and medical images.

Mainly RS same scale are date, no foregroud, background data.
Computer vision not good close and fare person.

** Take aways
U-Net as Enc-dec.
Medical and RGB data.

* From CNNs to Vision Transformers
- 1950
- 1990
- 2015
- 2020s Transformers!
More universal, easy to overfit.

Back to convolution. Too restricted.
Big benefit to self attention. Neighboring are more relevant.
Need more training data!!!!

** Vision Transformer models (ViT)
Split image to patches -> patch embedding trafo to tokens. List of
input tokens. tip 512 long. ViT model black-box input token output token is
identifiable. Then MLP classifier. Benefit everything encoded as a token.

Originally for languages.

*** Underline machanism
Keys, Queries, Values

Self attention. Take tokens row to matrix. Three linear trafo. Three
different feature matrices. Every query for similarities.

Next step one lin trafo multiply keys. Attention score. Every input to output.
Linear transformation.,

Language every token is a word, How relevant the keys are. Attention matrix.

Three blue one brown 40 mins video explanation.

Attention matrix. Connect input tokens to outputtokens.

Language sparse matrix, but picture diagonal, because spatial connection.

Keys, Queries, Values

V values as a multiplier.

All together. Nicely programmable.

** Multi-head Self-Attention (MHSA)
Contecaneted or stacked. Tokens to heads. Staked output reduced.
Same input and output side. Multi head

** MHSA & feed forward
Changes the features.

MHSA mixes features in a dimension, FeedForward MLP processes tokens separately.

** Full architecture
Original paper.

Repeated several times.

** Overview
form 1950-2020

** Underlying lin trafo
Weight to nonlinearities.

* Break 12-13

Itt mentem az állomásra.

* Probs Distrib
Important I.I.D.

Models 

** Balancing Underfitting/Overfitting
More data better model.

complexity vs.
Regularization

* Break 14:30

* Practice
https://tinyurl.com/eduhun1
https://tinyurl.com/eduhun1solution

* <2025-08-28 cs> gyakorlat
https://tinyurl.com/eduhun2
https://tinyurl.com/eduhun2solution
Utóbbi:
https://colab.research.google.com/drive/1w321OVT9ybgqzyek4BN19UTyitXGwetn?usp=sharing

Bevezetés.

#+begin_src sh
apt install rasterio
apt install python3-pip
#+end_sh

Main addition for custom dataset.
Pytorch dataset class.

Init function.

wake_decay to Adam optimizer

no gredient because video ram?

3 végén sample_idx

Unet standard for segmentation. Model from yesterday.
